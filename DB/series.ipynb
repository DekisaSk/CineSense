{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Korisnik\\Documents\\GitHub\\CineSense\\BackEnd\n"
     ]
    }
   ],
   "source": [
    "%cd ../BackEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import insert\n",
    "from sqlalchemy.dialects.postgresql import insert as pg_insert\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "async def insert_genres_with_on_conflict(db, new_genres_for_bulk):\n",
    "\n",
    "    if not new_genres_for_bulk:\n",
    "        return  # nothing to do\n",
    "\n",
    "    try:\n",
    "        # 1) Build the INSERT statement using the PostgreSQL dialect\n",
    "        stmt = pg_insert(Genre).values(new_genres_for_bulk)\n",
    "\n",
    "        # 2) Apply on_conflict_do_nothing to skip duplicates\n",
    "        stmt = stmt.on_conflict_do_nothing(\n",
    "            index_elements=['genre_id']  # which column to check for conflict\n",
    "        )\n",
    "\n",
    "        # 3) Execute and commit\n",
    "        await db.execute(stmt)\n",
    "        await db.commit()\n",
    "        print(f\"Inserted {len(new_genres_for_bulk)} genre(s) with upsert logic (duplicate keys ignored).\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        await db.rollback()\n",
    "        print(f\"Insert failed due to SQLAlchemyError: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def safe_str(val):\n",
    "\n",
    "    if isinstance(val, float) and math.isnan(val):\n",
    "        return None\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def parse_date(date_str):\n",
    "    if not date_str or pd.isna(date_str):\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
    "    except ValueError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk #1 with 10000 rows...\n",
      "Processing chunk #2 with 10000 rows...\n",
      "Processing chunk #3 with 10000 rows...\n",
      "Processing chunk #4 with 10000 rows...\n",
      "Processing chunk #5 with 10000 rows...\n",
      "Processing chunk #6 with 10000 rows...\n",
      "Processing chunk #7 with 10000 rows...\n",
      "Processing chunk #8 with 10000 rows...\n",
      "Processing chunk #9 with 10000 rows...\n",
      "Processing chunk #10 with 10000 rows...\n",
      "Processing chunk #11 with 10000 rows...\n",
      "Processing chunk #12 with 10000 rows...\n",
      "Processing chunk #13 with 10000 rows...\n",
      "Processing chunk #14 with 10000 rows...\n",
      "Processing chunk #15 with 10000 rows...\n",
      "Processing chunk #16 with 10000 rows...\n",
      "Processing chunk #17 with 10000 rows...\n",
      "Processing chunk #18 with 10000 rows...\n",
      "Processing chunk #19 with 10000 rows...\n",
      "Processing chunk #20 with 6504 rows...\n",
      "Finished writing TV temp CSV file.\n",
      "COPYing tv_shows from temp CSV...\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "TV_TEMP_CSV = \"tv_shows_temp.csv\"\n",
    "# ...similar for GENRES_TEMP_CSV, BRIDGING_TEMP_CSV if needed...\n",
    "\n",
    "def safe_str(val):\n",
    "    if val is None or (isinstance(val, float) and math.isnan(val)):\n",
    "        return \"\"\n",
    "    return str(val)\n",
    "\n",
    "def parse_date(val):\n",
    "    # Your date parsing or just safe_str if you want the DB to parse \"YYYY-MM-DD\"\n",
    "    if pd.isna(val):\n",
    "        return \"\"\n",
    "    return str(val)\n",
    "\n",
    "def load_tv_copy(csv_path: str, batch_size: int = 10000):\n",
    "    chunk_iter = pd.read_csv(\n",
    "        csv_path,\n",
    "        chunksize=batch_size,\n",
    "        sep=\",\",\n",
    "        encoding=\"utf-8\",\n",
    "        quotechar='\"'\n",
    "    )\n",
    "\n",
    "    # Remove old temp file if it exists\n",
    "    if os.path.exists(TV_TEMP_CSV):\n",
    "        os.remove(TV_TEMP_CSV)\n",
    "\n",
    "    # Open file in append mode but using csv.writer\n",
    "    with open(TV_TEMP_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as tv_file:\n",
    "        writer = csv.writer(tv_file, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        # Optional: Write header row if you want COPY to read header\n",
    "        writer.writerow([\n",
    "            \"tmdb_id\",\n",
    "            \"name\",\n",
    "            \"original_name\",\n",
    "            \"overview\",\n",
    "            \"tagline\",\n",
    "            \"first_air_date\",\n",
    "            \"popularity\",\n",
    "            \"vote_average\",\n",
    "            \"vote_count\",\n",
    "            \"poster_path\",\n",
    "            \"backdrop_path\",\n",
    "            \"type\"\n",
    "        ])\n",
    "\n",
    "        chunk_count = 0\n",
    "        for chunk_df in chunk_iter:\n",
    "            chunk_count += 1\n",
    "            print(f\"Processing chunk #{chunk_count} with {len(chunk_df)} rows...\")\n",
    "\n",
    "            for _, row in chunk_df.iterrows():\n",
    "                tmdb_id = row[\"id\"]  # or row.get(\"id\")\n",
    "                out_line = [\n",
    "                    safe_str(tmdb_id),\n",
    "                    safe_str(row.get(\"name\")),\n",
    "                    safe_str(row.get(\"original_name\")),\n",
    "                    safe_str(row.get(\"overview\")),\n",
    "                    safe_str(row.get(\"tagline\")),\n",
    "                    parse_date(row.get(\"first_air_date\")),\n",
    "                    safe_str(row.get(\"popularity\")),\n",
    "                    safe_str(row.get(\"vote_average\")),\n",
    "                    safe_str(row.get(\"vote_count\")),\n",
    "                    safe_str(row.get(\"poster_path\")),\n",
    "                    safe_str(row.get(\"backdrop_path\")),\n",
    "                    safe_str(row.get(\"type\")),\n",
    "                ]\n",
    "                writer.writerow(out_line)\n",
    "\n",
    "    print(\"Finished writing TV temp CSV file.\")\n",
    "\n",
    "    # Now do the COPY\n",
    "    conn = psycopg2.connect(\"dbname=cinesense_db user=admin password=dzuver host=45.155.126.141 port=5432\")\n",
    "    conn.autocommit = False\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            print(\"COPYing tv_shows from temp CSV...\")\n",
    "            copy_sql_tv = \"\"\"\n",
    "                COPY tv_shows (\n",
    "                    tmdb_id,\n",
    "                    name,\n",
    "                    original_name,\n",
    "                    overview,\n",
    "                    tagline,\n",
    "                    first_air_date,\n",
    "                    popularity,\n",
    "                    vote_average,\n",
    "                    vote_count,\n",
    "                    poster_path,\n",
    "                    backdrop_path,\n",
    "                    type\n",
    "                )\n",
    "                FROM STDIN WITH (FORMAT csv, HEADER true, QUOTE '\"', ESCAPE '\"')\n",
    "            \"\"\"\n",
    "            with open(TV_TEMP_CSV, \"r\", encoding=\"utf-8\") as f:\n",
    "                cur.copy_expert(copy_sql_tv, f)\n",
    "\n",
    "            conn.commit()\n",
    "            print(\"All COPY operations committed successfully.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error during COPY: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "        print(\"DB connection closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_tv_copy(\"tv.csv\", batch_size=10000)\n",
    "    print(\"TV import complete via COPY!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
