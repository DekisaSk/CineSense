{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../BackEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import sqlalchemy as sql\n",
    "from sqlalchemy.orm import Session\n",
    "from database import SessionLocal\n",
    "from models.tv_show import TVShow, tv_genres\n",
    "from models.genre import Genre\n",
    "\n",
    "%cd ../DB\n",
    "\n",
    "def load_tv_bulk(csv_path: str, batch_size: int = 1000):\n",
    "\n",
    "\n",
    "    # Local caches to avoid duplicates\n",
    "    local_genres_cache = {}\n",
    "    new_genres_for_bulk = []\n",
    "\n",
    "    # For bridging table tv_genres: we accumulate (tmdb_id, genre_id)\n",
    "    bridging_rows = []\n",
    "\n",
    "    chunk_iter = pd.read_csv(\n",
    "        csv_path,\n",
    "        chunksize=batch_size,\n",
    "        sep=\",\",\n",
    "        encoding=\"utf-8\",\n",
    "        quotechar='\"'\n",
    "    )\n",
    "\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        for chunk_index, chunk_df in enumerate(chunk_iter, start=1):\n",
    "            print(f\"Processing chunk #{chunk_index} with {len(chunk_df)} rows...\")\n",
    "\n",
    "            # accumulator for bulk insert\n",
    "            tvshows_to_insert = []\n",
    "\n",
    "            for _, row in chunk_df.iterrows():\n",
    "                # Parse or skip the date\n",
    "                if \"first_air_date\" in row and pd.notna(row[\"first_air_date\"]):\n",
    "                    first_air = row[\"first_air_date\"]\n",
    "                else:\n",
    "                    first_air = None\n",
    "\n",
    "                \n",
    "\n",
    "                tmdb_id = row[\"id\"]\n",
    "\n",
    "                tvshows_to_insert.append({\n",
    "                    \"tmdb_id\":         tmdb_id,\n",
    "                    \"name\":            row.get(\"name\"),\n",
    "                    \"original_name\":   row.get(\"original_name\"),\n",
    "                    \"overview\":        row.get(\"overview\"),\n",
    "                    \"tagline\":         row.get(\"tagline\"),\n",
    "                    \"first_air_date\":  first_air,\n",
    "                    \"popularity\":      row.get(\"popularity\"),\n",
    "                    \"vote_average\":    row.get(\"vote_average\"),\n",
    "                    \"vote_count\":      row.get(\"vote_count\"),\n",
    "                    \"poster_path\":     row.get(\"poster_path\"),\n",
    "                    \"backdrop_path\":   row.get(\"backdrop_path\"),\n",
    "                    \"type\":            row.get(\"type\"),\n",
    "                })\n",
    "\n",
    "                # Parse the genres from JSON in the \"genres\" column\n",
    "                if \"genres\" in row and pd.notna(row[\"genres\"]):\n",
    "                    try:\n",
    "                        genre_data = json.loads(row[\"genres\"])\n",
    "                        for g in genre_data:\n",
    "                            gid = g[\"id\"]\n",
    "                            # if not within cache use \n",
    "                            if gid not in local_genres_cache:\n",
    "                                local_genres_cache[gid] = True\n",
    "                                new_genres_for_bulk.append({\n",
    "                                    \"genre_id\": gid,\n",
    "                                    \"name\":     g[\"name\"]\n",
    "                                })\n",
    "\n",
    "                            # build bridging\n",
    "                            bridging_rows.append({\n",
    "                                \"tmdb_id\":  tmdb_id,\n",
    "                                \"genre_id\": gid\n",
    "                            })\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "\n",
    "            # instert chunks of genres\n",
    "            if new_genres_for_bulk:\n",
    "                print(f\"Inserting {len(new_genres_for_bulk)} new genres discovered...\")\n",
    "                db.bulk_insert_mappings(Genre, new_genres_for_bulk)\n",
    "                db.commit()\n",
    "                new_genres_for_bulk.clear()\n",
    "                print(\"New genres inserted and committed.\")\n",
    "\n",
    "            # Insert chunk of TV shows\n",
    "            if tvshows_to_insert:\n",
    "                print(f\"Bulk inserting {len(tvshows_to_insert)} tv shows for chunk #{chunk_index}...\")\n",
    "                db.bulk_insert_mappings(TVShow, tvshows_to_insert)\n",
    "                db.commit()\n",
    "                print(\"TV shows inserted & committed.\")\n",
    "\n",
    "            # Insert bridging rows in tv_genres table, deduplicating\n",
    "            if bridging_rows:\n",
    "                print(f\"Bulk inserting {len(bridging_rows)} bridging rows for chunk #{chunk_index}...\")\n",
    "                # eliminating duplicates\n",
    "                unique_pairs = set()\n",
    "                for br in bridging_rows:\n",
    "                    unique_pairs.add((br[\"tmdb_id\"], br[\"genre_id\"]))\n",
    "\n",
    "                deduped_rows = [\n",
    "                    {\"tmdb_id\": pair[0], \"genre_id\": pair[1]}\n",
    "                    for pair in unique_pairs\n",
    "                ]\n",
    "\n",
    "                print(f\"Actually inserting {len(deduped_rows)} unique bridging rows after dedup...\")\n",
    "\n",
    "                insert_stmt = tv_genres.insert()\n",
    "                db.execute(insert_stmt, deduped_rows)\n",
    "                db.commit()\n",
    "                bridging_rows.clear()\n",
    "                print(\"Bridging rows inserted & committed.\")\n",
    "\n",
    "        print(\"All chunks processed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        db.rollback()\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_tv_bulk(\"tv.csv\", batch_size=100000)\n",
    "    print(\"TV import complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
