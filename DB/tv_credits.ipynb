{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../BackEnd\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from sqlalchemy.orm import Session\n",
    "from database import SessionLocal\n",
    "from models.credit import Credit\n",
    "from models.person import Person\n",
    "from models.cast_member import CastMember\n",
    "from models.crew_member import CrewMember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3553674 persons, 14123007 credits, 7317625 cast entries, and 6805382 crew entries from existing CSVs.\n",
      "Processing TV chunk #1 with 100000 rows...\n",
      "Processing TV chunk #2 with 96692 rows...\n",
      "TV credits CSV files appended with unique data.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# CSV file names (these are the same files used for movie credits)\n",
    "PERSONS_CSV = \"persons_temp.csv\"\n",
    "CREDITS_CSV = \"credits_temp.csv\"\n",
    "CAST_CSV    = \"cast_temp.csv\"\n",
    "CREW_CSV    = \"crew_temp.csv\"\n",
    "\n",
    "# Utility functions to load existing keys/pairs from a CSV file\n",
    "def load_existing_keys(filename, key_index=0):\n",
    "    keys = set()\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader, None)  # skip header\n",
    "            for row in reader:\n",
    "                if row and len(row) > key_index:\n",
    "                    keys.add(row[key_index])\n",
    "    return keys\n",
    "\n",
    "def load_existing_pairs(filename, key_index1=0, key_index2=1):\n",
    "    pairs = set()\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader, None)  # skip header\n",
    "            for row in reader:\n",
    "                if row and len(row) > key_index2:\n",
    "                    pairs.add((row[key_index1], row[key_index2]))\n",
    "    return pairs\n",
    "\n",
    "# Load global caches from existing CSV files (if any)\n",
    "global_persons_cache = load_existing_keys(PERSONS_CSV, key_index=0)\n",
    "global_credits_cache = load_existing_keys(CREDITS_CSV, key_index=0)\n",
    "global_cast_cache    = load_existing_pairs(CAST_CSV, key_index1=0, key_index2=1)  # (credit_id, person_id)\n",
    "global_crew_cache    = load_existing_pairs(CREW_CSV, key_index1=0, key_index2=1)  # (credit_id, person_id)\n",
    "\n",
    "print(f\"Loaded {len(global_persons_cache)} persons, {len(global_credits_cache)} credits, \"\n",
    "      f\"{len(global_cast_cache)} cast entries, and {len(global_crew_cache)} crew entries from existing CSVs.\")\n",
    "\n",
    "# Open CSV files in append mode (so that new TV data gets added after movie data)\n",
    "persons_file = open(PERSONS_CSV, \"a\", newline=\"\", encoding=\"utf-8\")\n",
    "credits_file = open(CREDITS_CSV, \"a\", newline=\"\", encoding=\"utf-8\")\n",
    "cast_file    = open(CAST_CSV, \"a\", newline=\"\", encoding=\"utf-8\")\n",
    "crew_file    = open(CREW_CSV, \"a\", newline=\"\", encoding=\"utf-8\")\n",
    "\n",
    "persons_writer = csv.writer(persons_file)\n",
    "credits_writer = csv.writer(credits_file)\n",
    "cast_writer    = csv.writer(cast_file)\n",
    "crew_writer    = csv.writer(crew_file)\n",
    "\n",
    "# (Assumes the CSVs already have header rows from the movie import.)\n",
    "\n",
    "# Process the TV credits CSV file in chunks.\n",
    "chunk_iter = pd.read_csv(\n",
    "    \"tv_credits.csv\",\n",
    "    chunksize=100000,\n",
    "    sep=\",\",\n",
    "    encoding=\"utf-8\",\n",
    "    quotechar='\"'\n",
    ")\n",
    "\n",
    "chunk_count = 0\n",
    "for chunk_df in chunk_iter:\n",
    "    chunk_count += 1\n",
    "    print(f\"Processing TV chunk #{chunk_count} with {len(chunk_df)} rows...\")\n",
    "    # Local caches for this chunk (to minimize repeated writes within the chunk)\n",
    "    local_persons_cache = set()\n",
    "    local_credits_cache = set()\n",
    "\n",
    "    for _, row in chunk_df.iterrows():\n",
    "        content_id = row[\"id\"]  # TV TMDB ID\n",
    "\n",
    "        # Parse cast JSON\n",
    "        cast_data = []\n",
    "        if \"cast\" in row and pd.notna(row[\"cast\"]):\n",
    "            try:\n",
    "                cast_data = json.loads(row[\"cast\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Parse crew JSON\n",
    "        crew_data = []\n",
    "        if \"crew\" in row and pd.notna(row[\"crew\"]):\n",
    "            try:\n",
    "                crew_data = json.loads(row[\"crew\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Process cast members\n",
    "        for member in cast_data:\n",
    "            credit_id = member.get(\"credit_id\")\n",
    "            person_id = member.get(\"id\")\n",
    "            if not credit_id or not person_id:\n",
    "                continue\n",
    "            # Convert IDs to strings for consistency\n",
    "            credit_id = str(credit_id)\n",
    "            person_id = str(person_id)\n",
    "\n",
    "            # Add person if new (check both global and local caches)\n",
    "            if person_id not in global_persons_cache and person_id not in local_persons_cache:\n",
    "                local_persons_cache.add(person_id)\n",
    "                persons_writer.writerow([\n",
    "                    person_id,\n",
    "                    member.get(\"name\", \"\"),\n",
    "                    member.get(\"profile_path\", \"\")\n",
    "                ])\n",
    "\n",
    "            # Add credit if new\n",
    "            if credit_id not in global_credits_cache and credit_id not in local_credits_cache:\n",
    "                local_credits_cache.add(credit_id)\n",
    "                credits_writer.writerow([credit_id, content_id, \"tv\"])\n",
    "\n",
    "            # Add cast entry if new (using (credit_id, person_id) as unique key)\n",
    "            cast_pair = (credit_id, person_id)\n",
    "            if cast_pair not in global_cast_cache:\n",
    "                global_cast_cache.add(cast_pair)\n",
    "                cast_writer.writerow([\n",
    "                    credit_id,\n",
    "                    person_id,\n",
    "                    member.get(\"character\", \"\"),\n",
    "                    member.get(\"order\", \"\")\n",
    "                ])\n",
    "\n",
    "        # Process crew members\n",
    "        for member in crew_data:\n",
    "            credit_id = member.get(\"credit_id\")\n",
    "            person_id = member.get(\"id\")\n",
    "            if not credit_id or not person_id:\n",
    "                continue\n",
    "            credit_id = str(credit_id)\n",
    "            person_id = str(person_id)\n",
    "\n",
    "            if person_id not in global_persons_cache and person_id not in local_persons_cache:\n",
    "                local_persons_cache.add(person_id)\n",
    "                persons_writer.writerow([\n",
    "                    person_id,\n",
    "                    member.get(\"name\", \"\"),\n",
    "                    member.get(\"profile_path\", \"\")\n",
    "                ])\n",
    "\n",
    "            if credit_id not in global_credits_cache and credit_id not in local_credits_cache:\n",
    "                local_credits_cache.add(credit_id)\n",
    "                credits_writer.writerow([credit_id, content_id, \"tv\"])\n",
    "\n",
    "            crew_pair = (credit_id, person_id)\n",
    "            if crew_pair not in global_crew_cache:\n",
    "                global_crew_cache.add(crew_pair)\n",
    "                crew_writer.writerow([\n",
    "                    credit_id,\n",
    "                    person_id,\n",
    "                    member.get(\"department\", \"\"),\n",
    "                    member.get(\"job\", \"\")\n",
    "                ])\n",
    "\n",
    "    # Update global caches with new keys found in this chunk.\n",
    "    global_persons_cache.update(local_persons_cache)\n",
    "    global_credits_cache.update(local_credits_cache)\n",
    "\n",
    "# Close all files.\n",
    "persons_file.close()\n",
    "credits_file.close()\n",
    "cast_file.close()\n",
    "crew_file.close()\n",
    "\n",
    "print(\"TV credits CSV files appended with unique data.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
